# ‚ö° Guia de Otimiza√ß√£o de Velocidade

## üöÄ 3 Formas de Acelerar o Chatbot

### üî• **OP√á√ÉO 1: GROQ (RECOMENDADO - 10x MAIS R√ÅPIDO)**

**Velocidade**: 500+ tokens/segundo ‚ö°‚ö°‚ö°  
**Qualidade**: Excelente (Llama 3.1 70B)  
**Custo**: GRATUITO (com limites generosos)

#### Como Ativar:

1. **Obtenha sua API Key**:
   - Acesse: https://console.groq.com
   - Crie uma conta (gratuita)
   - Gere uma API Key

2. **Instale o Groq**:
```bash
pip install groq
```

3. **Configure no `config.py`**:
```python
# Adicione sua chave
GROQ_API_KEY = "gsk_..."  # Sua chave aqui

# Descomente as linhas:
LLM_PROVIDER = "groq"
MODEL_NAME = "llama-3.1-70b-versatile"
```

4. **Execute normalmente**:
```bash
streamlit run chatbot_advanced.py
```

‚úÖ **Resultado**: Respostas em **2-4 segundos** (vs 15-30 segundos com Gemini Pro)

---

### ‚ö° **OP√á√ÉO 2: GEMINI FLASH (3x MAIS R√ÅPIDO)**

**Velocidade**: ~100-150 tokens/segundo  
**Qualidade**: Boa (inferior ao Pro, mas suficiente)  
**Custo**: Mais barato que Pro

#### Como Ativar:

No `config.py`, altere:
```python
# Mantenha Gemini mas use Flash
LLM_PROVIDER = "gemini"
MODEL_NAME = "gemini-2.0-flash-exp"
```

‚úÖ **Resultado**: Respostas em **5-8 segundos**

---

### üéØ **OP√á√ÉO 3: REDUZIR CHUNKS (Mais R√°pido mas Menos Contexto)**

**Velocidade**: Marginal (~20% mais r√°pido)  
**Qualidade**: Pode perder contexto relevante  

#### Como Ativar:

No `config.py`, ajuste:
```python
RAG_CONFIG = {
    "retrieval_top_k": 50,   # Era 100
    "rerank_top_n": 5,       # Era 10
}
```

‚ö†Ô∏è **Trade-off**: Menos chunks = respostas mais r√°pidas mas potencialmente menos completas

---

## üìä Compara√ß√£o de Velocidade

| Op√ß√£o | Velocidade (tokens/s) | Tempo Resposta | Qualidade | Custo |
|-------|----------------------|----------------|-----------|-------|
| **Groq Llama 3.1 70B** | 500+ | 2-4s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Gr√°tis |
| **Groq Mixtral 8x7B** | 600+ | 1-3s | ‚≠ê‚≠ê‚≠ê‚≠ê | Gr√°tis |
| **Gemini Flash** | 100-150 | 5-8s | ‚≠ê‚≠ê‚≠ê‚≠ê | $ |
| **Gemini Pro** | 30-50 | 15-30s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $$ |

---

## üéØ Recomenda√ß√£o Final

### Para M√ÅXIMA VELOCIDADE:
```python
# config.py
GROQ_API_KEY = "sua_chave"
LLM_PROVIDER = "groq"
MODEL_NAME = "llama-3.1-70b-versatile"

RAG_CONFIG = {
    "retrieval_top_k": 50,
    "rerank_top_n": 5,
}
```

**Resultado**: Respostas em **1-3 segundos** mantendo excelente qualidade! ‚ö°üöÄ

---

## üîç Modelos Groq Dispon√≠veis

| Modelo | Velocidade | Qualidade | Contexto |
|--------|-----------|-----------|----------|
| `llama-3.3-70b-versatile` | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 128K tokens |
| `llama-3.1-70b-versatile` | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 128K tokens |
| `mixtral-8x7b-32768` | ‚ö°‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | 32K tokens |

**Recomendado**: `llama-3.3-70b-versatile` (mais recente e poderoso)

---

## ‚ùì FAQ

**P: Groq √© realmente gratuito?**  
R: Sim! Com limites generosos (6000 tokens/minuto para Llama 70B).

**P: Preciso trocar todo o c√≥digo?**  
R: N√£o! Apenas configure no `config.py` e pronto.

**P: Groq funciona offline?**  
R: N√£o, √© uma API online como Gemini.

**P: Posso usar Groq + Gemini?**  
R: Sim! O c√≥digo tem fallback autom√°tico: se Groq falhar, usa Gemini.

**P: Qual a diferen√ßa de qualidade Groq vs Gemini Pro?**  
R: Na pr√°tica, para este caso de uso (RAG), a qualidade √© **equivalente**.

---

## üöÄ Conclus√£o

Para **M√ÅXIMA VELOCIDADE SEM PERDER QUALIDADE**:

1. ‚úÖ Use **GROQ** com **Llama 3.3 70B**
2. ‚úÖ Configure `retrieval_top_k: 50` e `rerank_top_n: 5`
3. ‚úÖ Mantenha Cohere reranker ativo

**Resultado**: Sistema **10x mais r√°pido** mantendo respostas excelentes! ‚ö°üéØ

