#!/usr/bin/env python3
"""
Indexador Avan√ßado com Suporte a Imagens
==========================================
Processa PDFs, extrai texto e imagens, cria embeddings e banco vetorial
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Tuple
import hashlib

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from pypdf import PdfReader
import fitz  # PyMuPDF para extra√ß√£o de imagens reais
from PIL import Image
import io

from config import GOOGLE_API_KEY
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

console = Console()


class AdvancedPDFProcessor:
    """Processador avan√ßado de PDFs com extra√ß√£o de imagens"""
    
    def __init__(self, docs_path: str):
        self.docs_path = Path(docs_path)
        self.images_dir = Path("extracted_images")
        self.images_dir.mkdir(exist_ok=True)
        self.image_metadata = {}
        
    def extract_images_from_pdf(self, pdf_path: Path) -> Dict[int, List[str]]:
        """Extrai IMAGENS REAIS de um PDF (diagramas, gr√°ficos, etc) e salva localmente"""
        console.print(f"  [yellow]üì∑[/yellow] Extraindo imagens REAIS de {pdf_path.name}...")
        
        images_by_page = {}
        total_images = 0
        
        try:
            # Abre PDF com PyMuPDF
            pdf_document = fitz.open(str(pdf_path))
            
            for page_num in range(len(pdf_document)):
                page = pdf_document[page_num]
                page_number = page_num + 1  # 1-indexed
                
                # Obt√©m lista de imagens na p√°gina
                image_list = page.get_images(full=True)
                
                if not image_list:
                    continue  # Sem imagens nesta p√°gina
                
                if page_number not in images_by_page:
                    images_by_page[page_number] = []
                
                # Extrai cada imagem
                for img_index, img in enumerate(image_list):
                    try:
                        xref = img[0]  # Refer√™ncia da imagem
                        
                        # Extrai a imagem
                        base_image = pdf_document.extract_image(xref)
                        image_bytes = base_image["image"]
                        image_ext = base_image["ext"]
                        
                        # Nome do arquivo
                        img_filename = f"{pdf_path.stem}_p{page_number}_img{img_index}.{image_ext}"
                        img_path = self.images_dir / img_filename
                        
                        # Salva imagem
                        with open(img_path, "wb") as img_file:
                            img_file.write(image_bytes)
                        
                        # Converte para PNG se necess√°rio (para consist√™ncia)
                        if image_ext != "png":
                            img = Image.open(img_path)
                            png_filename = f"{pdf_path.stem}_p{page_number}_img{img_index}.png"
                            png_path = self.images_dir / png_filename
                            img.save(png_path, "PNG")
                            img_path.unlink()  # Remove original
                            img_path = png_path
                            img_filename = png_filename
                        
                        images_by_page[page_number].append(str(img_path))
                        total_images += 1
                        
                        # Armazena metadata
                        self.image_metadata[img_filename] = {
                            'document': pdf_path.stem,
                            'page': page_number,
                            'path': str(img_path),
                            'index': img_index
                        }
                        
                    except Exception as e:
                        console.print(f"    [dim]‚ö†Ô∏è  Erro ao extrair imagem {img_index} da p√°gina {page_number}: {e}[/dim]")
                        continue
            
            pdf_document.close()
            
            if total_images > 0:
                console.print(f"    ‚úì {total_images} imagens REAIS extra√≠das de {len(images_by_page)} p√°ginas")
            else:
                console.print(f"    [dim]‚ö†Ô∏è  Nenhuma imagem encontrada neste PDF[/dim]")
            
        except Exception as e:
            console.print(f"    [red]‚úó[/red] Erro ao processar PDF: {e}")
        
        return images_by_page
    
    def process_pdf(self, pdf_path: Path) -> Tuple[List[Document], Dict]:
        """Processa um PDF: extrai texto, metadados e imagens"""
        console.print(f"\n[cyan]üìÑ Processando:[/cyan] {pdf_path.name}")
        
        documents = []
        
        try:
            # Extrai imagens
            images_by_page = self.extract_images_from_pdf(pdf_path)
            
            # L√™ PDF
            reader = PdfReader(str(pdf_path))
            
            for page_num, page in enumerate(reader.pages, start=1):
                text = page.extract_text()
                
                if text.strip():
                    # Identifica se h√° imagens nesta p√°gina
                    page_images = images_by_page.get(page_num, [])
                    
                    doc = Document(
                        page_content=text,
                        metadata={
                            'source': pdf_path.stem,
                            'page': page_num,
                            'total_pages': len(reader.pages),
                            'has_images': len(page_images) > 0,
                            'images': json.dumps(page_images),  # ‚Üê Converte lista para JSON string
                            'section': self._identify_section(text)
                        }
                    )
                    documents.append(doc)
            
            console.print(f"  [green]‚úì[/green] {len(documents)} p√°ginas processadas")
            
        except Exception as e:
            console.print(f"  [red]‚úó[/red] Erro: {e}")
        
        return documents, images_by_page
    
    def _identify_section(self, text: str) -> str:
        """Identifica a se√ß√£o do documento baseado no texto"""
        text_lower = text.lower()
        
        # Palavras-chave para identificar se√ß√µes
        sections = {
            'introduction': ['introduction', 'introdu√ß√£o', 'overview'],
            'architecture': ['architecture', 'arquitetura', 'design'],
            'migration': ['migration', 'migra√ß√£o', 'migrating'],
            'code_conversion': ['code conversion', 'convers√£o de c√≥digo', 'converting code'],
            'data_migration': ['data migr', 'migra√ß√£o de dados', 'data migrator'],
            'tools': ['tools', 'ferramentas', 'tooling'],
            'best_practices': ['best practices', 'boas pr√°ticas', 'recommendations'],
            'concepts': ['concept', 'conceito', 'fundamental'],
        }
        
        for section_name, keywords in sections.items():
            if any(keyword in text_lower for keyword in keywords):
                return section_name
        
        return 'general'


class AdvancedIndexer:
    """Indexador avan√ßado com embeddings Google e ChromaDB"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=api_key
        )
        self.vectorstore = None
        self.processor = AdvancedPDFProcessor("documenta√ß√£o_migracao_camunda")
        
    def create_chunks(self, documents: List[Document]) -> List[Document]:
        """Cria chunks dos documentos com overlap"""
        console.print("\n[cyan]‚úÇÔ∏è  Criando chunks...[/cyan]")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # Tamanho do chunk
            chunk_overlap=200,  # Overlap para manter contexto
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        
        # Adiciona ID √∫nico a cada chunk
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = f"chunk_{i}"
            chunk.metadata['chunk_index'] = i
        
        console.print(f"  [green]‚úì[/green] {len(chunks)} chunks criados")
        return chunks
    
    def build_vectorstore(self, chunks: List[Document]) -> Chroma:
        """Cria banco vetorial com ChromaDB"""
        console.print("\n[cyan]üóÑÔ∏è  Criando banco vetorial...[/cyan]")
        
        persist_directory = "./chroma_db"
        
        try:
            vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=persist_directory,
                collection_name="camunda_migration"
            )
            
            console.print(f"  [green]‚úì[/green] Banco vetorial criado em {persist_directory}")
            console.print(f"  [green]‚úì[/green] {len(chunks)} chunks indexados")
            
            return vectorstore
            
        except Exception as e:
            console.print(f"  [red]‚úó[/red] Erro: {e}")
            raise
    
    def index_all_documents(self):
        """Indexa todos os PDFs da documenta√ß√£o"""
        console.print("\n[bold cyan]üöÄ Iniciando Indexa√ß√£o Avan√ßada[/bold cyan]")
        console.print("="*70 + "\n")
        
        pdf_files = list(self.processor.docs_path.glob("*.pdf"))
        
        if not pdf_files:
            console.print("[red]‚ùå Nenhum PDF encontrado![/red]")
            return
        
        console.print(f"üìö Encontrados {len(pdf_files)} documentos\n")
        
        all_documents = []
        all_images_metadata = {}
        
        # Processa cada PDF
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console
        ) as progress:
            
            task = progress.add_task("Processando PDFs...", total=len(pdf_files))
            
            for pdf_file in pdf_files:
                docs, images = self.processor.process_pdf(pdf_file)
                all_documents.extend(docs)
                all_images_metadata.update(self.processor.image_metadata)
                progress.advance(task)
        
        # Salva metadata de imagens
        with open("image_metadata.json", "w") as f:
            json.dump(all_images_metadata, f, indent=2)
        
        console.print(f"\n[green]‚úì[/green] Total: {len(all_documents)} documentos processados")
        
        # Cria chunks
        chunks = self.create_chunks(all_documents)
        
        # Cria embeddings e vectorstore
        self.vectorstore = self.build_vectorstore(chunks)
        
        console.print("\n" + "="*70)
        console.print("[bold green]‚úÖ INDEXA√á√ÉO CONCLU√çDA COM SUCESSO![/bold green]")
        console.print("="*70 + "\n")
        
        # Estat√≠sticas
        self._print_statistics(all_documents, chunks, all_images_metadata)
    
    def _print_statistics(self, documents, chunks, images_metadata):
        """Imprime estat√≠sticas da indexa√ß√£o"""
        console.print("[bold]üìä Estat√≠sticas:[/bold]\n")
        
        # Documentos
        doc_sources = set(doc.metadata['source'] for doc in documents)
        console.print(f"  üìÑ Documentos √∫nicos: {len(doc_sources)}")
        console.print(f"  üìÉ Total de p√°ginas: {len(documents)}")
        console.print(f"  ‚úÇÔ∏è  Total de chunks: {len(chunks)}")
        
        # Imagens
        console.print(f"  üì∑ Total de imagens: {len(images_metadata)}")
        
        chunks_with_images = sum(1 for chunk in chunks if chunk.metadata.get('has_images'))
        console.print(f"  üñºÔ∏è  Chunks com imagens: {chunks_with_images}")
        
        # Se√ß√µes
        sections = {}
        for doc in documents:
            section = doc.metadata.get('section', 'unknown')
            sections[section] = sections.get(section, 0) + 1
        
        console.print(f"\n  üìë P√°ginas por se√ß√£o:")
        for section, count in sorted(sections.items(), key=lambda x: x[1], reverse=True):
            console.print(f"     ‚Ä¢ {section}: {count}")
        
        console.print()


def main():
    """Fun√ß√£o principal"""
    
    # Verifica API key
    if not GOOGLE_API_KEY:
        console.print("[red]‚ùå GOOGLE_API_KEY n√£o configurada no config.py![/red]")
        return
    
    # Cria indexador
    indexer = AdvancedIndexer(GOOGLE_API_KEY)
    
    # Executa indexa√ß√£o
    try:
        indexer.index_all_documents()
        
        console.print("[bold green]üéâ Pronto! Execute o chatbot avan√ßado:[/bold green]")
        console.print("[cyan]   streamlit run chatbot_advanced.py[/cyan]\n")
        
    except Exception as e:
        console.print(f"\n[bold red]‚ùå Erro durante indexa√ß√£o: {e}[/bold red]")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

